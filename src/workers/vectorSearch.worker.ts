import { db } from '../services/indexedDB/db';
import { cosineSimilarity, calculateVectorNorm } from '../utils/vectorUtils';
import type { VectorSearchResult, EmbeddingChunk } from '../types';

// Define worker message types
type WorkerMessage =
    | { type: 'SEARCH', payload: { queryEmbedding: Float32Array; sessionId: string; options: any } }
    | { type: 'INIT' };

// Define worker response types
type WorkerResponse =
    | { type: 'SEARCH_RESULT', payload: VectorSearchResult[] }
    | { type: 'ERROR', error: string };

const ctx: Worker = self as any;

// Handle messages from main thread
ctx.onmessage = async (event: MessageEvent<WorkerMessage>) => {
    const data = event.data;

    try {
        if (data.type === 'SEARCH') {
            console.log('ðŸ‘· [WORKER] Received Search Request', {
                sessionId: data.payload.sessionId,
                options: data.payload.options
            });
            const results = await performVectorSearch(
                data.payload.queryEmbedding,
                data.payload.sessionId,
                data.payload.options
            );
            console.log('ðŸ‘· [WORKER] Search Complete. Found results:', results.length);
            ctx.postMessage({ type: 'SEARCH_RESULT', payload: results });
        }
    } catch (error: any) {
        console.error('Vector Search Worker Error:', error);
        ctx.postMessage({ type: 'ERROR', error: error.message || 'Unknown worker error' });
    }
};

/**
 * Optimized Vector Search (Runs in Background Thread)
 * 1. Loads ONLY vectors (numbers) first -> Low Memory
 * 2. Calculates scores -> Heavy Math
 * 3. Loads FULL text only for top results -> Efficient
 */
async function performVectorSearch(
    queryEmbedding: Float32Array,
    sessionId: string,
    options: {
        maxResults?: number;
        similarityThreshold?: number;
        documentIds?: string[];
        userId?: string;
    }
): Promise<VectorSearchResult[]> {
    const {
        maxResults = 8,
        similarityThreshold = 0.7,
        documentIds,
        userId
    } = options;

    if (!db) {
        throw new Error('Database not initialized in worker');
    }

    // 1. Fetch Candidates (Optimized for Memory)
    // We only need embedding + ID + norm. We DO NOT load the full content yet.
    let candidateChunks: { id: string; documentId: string; embedding: Float32Array; embeddingNorm?: number }[] = [];

    if (documentIds && documentIds.length > 0) {
        // Specific docs - optimize with cursor to avoid loading content
        await db.embeddings
            .where('documentId')
            .anyOf(documentIds)
            .each(chunk => {
                candidateChunks.push({
                    id: chunk.id,
                    documentId: chunk.documentId,
                    embedding: chunk.embedding,
                    embeddingNorm: chunk.embeddingNorm
                });
            });
    } else {
        // Session docs
        // Get enabled document IDs first
        const enabledDocs = await db.documents
            .where('sessionId')
            .equals(sessionId)
            .and(doc => doc.enabled === true)
            .primaryKeys();

        if (enabledDocs.length === 0) return [];

        // Fetch embeddings for these docs using cursor optimization
        await db.embeddings
            .where('documentId')
            .anyOf(enabledDocs as string[])
            .each(chunk => {
                candidateChunks.push({
                    id: chunk.id,
                    documentId: chunk.documentId,
                    embedding: chunk.embedding,
                    embeddingNorm: chunk.embeddingNorm
                });
            });
    }

    // 2. Calculate Similarities (Heavy Math - OPTIMIZED)
    const results: { id: string; similarity: number }[] = [];

    // Pre-calculate query norm ONCE (saves thousands of recalculations)
    const queryNorm = calculateVectorNorm(queryEmbedding);
    console.log('ðŸ‘· [WORKER] Query norm calculated:', queryNorm.toFixed(4));

    for (const chunk of candidateChunks) {
        // Use pre-computed norms for both query and chunk
        const similarity = cosineSimilarity(
            queryEmbedding,
            chunk.embedding,
            queryNorm,
            chunk.embeddingNorm // From IndexedDB
        );
        if (similarity >= similarityThreshold) {
            results.push({ id: chunk.id, similarity });
        }
    }

    // 3. Sort & Adaptive Filter (The "Brain")
    results.sort((a, b) => b.similarity - a.similarity);

    // Apply Adaptive Cutoff
    if (results.length > 0) {
        const bestScore = results[0].similarity;
        const cutoff = bestScore * 0.85; // 15% drop-off rule

        // Filter in place
        let keepCount = 0;
        for (let i = 0; i < results.length; i++) {
            if (results[i].similarity >= cutoff) {
                keepCount++;
            } else {
                break; // Since sorted, we can stop early
            }
        }
        // Truncate to keep candidates + a buffer for page deduplication
        results.length = keepCount;
    }

    // 4. Hydrate Top Results (Load Content)
    // Only now do we fetch the heavy text content for the winners
    const topCandidateIds = results.map(r => r.id);
    const fullChunks = await db.embeddings.where('id').anyOf(topCandidateIds).toArray();

    // Need to map back to results with similarity
    const chunksMap = new Map(fullChunks.map(c => [c.id, c]));

    // Also need documents for titles
    const docIds = new Set(fullChunks.map(c => c.documentId));
    const documents = await db.documents.where('id').anyOf([...docIds]).toArray();
    const docMap = new Map(documents.map(d => [d.id, d]));

    const finalResults: VectorSearchResult[] = [];

    // Re-construct the full result objects
    for (const res of results) {
        const chunk = chunksMap.get(res.id);
        if (!chunk) continue;

        const doc = docMap.get(chunk.documentId);
        if (!doc) continue;

        finalResults.push({
            chunk,
            similarity: res.similarity,
            document: {
                id: doc.id,
                title: doc.title || doc.filename,
                fileName: doc.filename
            }
        });
    }

    // 5. Deduplicate by Page (Same logic as before, but simplified)
    // We can just return the raw results for now and let the service handle dedupe 
    // OR handle it here. Handling it here is better for main thread.
    // ... Implementing simple dedup logic ...

    // Return sorted
    return finalResults.sort((a, b) => b.similarity - a.similarity).slice(0, maxResults);
}
